{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "In D.L., our **objective** is, almost always, to find a **set of weights** that **minimizes error.** All of these sets of weights are **linear operations** and hence, if performed alone, we would attain just a simple **multiple linear regression model.** \n",
    "\n",
    "##### What’s the Problem with Linear Models?\n",
    "\n",
    "If inputs are left untouched, they are not **flexible** as they can only model linear relationships while most data out there has a **non-linear patterns.** Hence, we need to find a way to force our model to be able to **learn non-linear patterns.** \n",
    "\n",
    "##### How do we do this?\n",
    "\n",
    "After a set of linear operations, we apply to the new **input** created by the linear operations ($Ax = \\hat{y}$) a **non-linear activation function**.\n",
    "\n",
    "Suppose we have a simple linear model $\\hat{y}=ax+b$. These $\\hat{y}$ form a linear operation such as below:\n",
    "\n",
    "\n",
    "\n",
    "Well, given our orange line ($\\hat{y}$), we then apply a **non-linear activation function** so as to **transform** our linear model into a **fixed non-linear model** such as below:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Hoon_Chung2/publication/309775740/figure/fig1/AS:538049215381504@1505292337270/The-most-common-nonlinear-activation-functions.png\" alt=\"The most common nonlinear activation functions. | Download ...\" style=\"zoom: 50%;\" />\n",
    "\n",
    "**Why is this a fixed non-linear operation?**\n",
    "\n",
    "Because whatever formula we use for our non-linear operation, **we do NOT** have a set of weights on it that try to learn an optimal non-linear representation. It will always follow a **fixed, single transformation.**\n",
    "\n",
    "##### Well, isn’t our purpose to find an optimal non-linear operation?\n",
    "\n",
    "Yes and no. We find an optimal non-linear operation by letting our set of **linear weights** learn a **representation of the data** that, **once fed to the non-linear operation**, will **correctly identify the new pattern.** Hence, the objective of our linear weights now becomes to **find a representation of the data that, once fed to the non-linear activation, will correctly learn the non-linear patterns.**\n",
    "\n",
    "##### How do we backpropagate these non-linear activations?\n",
    "\n",
    "Given that these non-linear activations are in fact non-linear, we are unable to just take the input as the gradient as we can do with linear operations (3x => 3). Hence, we will need **two things**:\n",
    "\n",
    "1. The **input ($\\hat{y}$) that was fed to the non-linear activation** and\n",
    "2. The **derivative equation of the non-linear function.**\n",
    "\n",
    "Given that we want to apply the non-linear operation to every input, we can classify these operations as element-wise. \n",
    "\n",
    "This has important implications on how we can calculate our gradient. \n",
    "\n",
    "**First**, as we learned on the \"Linear Layer\" tutorial, the dimension of the incoming gradient from our subsequent operation will equal the dimension of the output from our non-linear operation. \n",
    "\n",
    "Now, since the output of the non-linear operations equals the dimension of the input, we are able to calculate the corresponding chain-gradient with a simple Hadamard product (element-wise multiplication) between our incoming gradient and our current non-linear operation. In other words,\n",
    "\n",
    "```input.shape == output.shape == incoming_grad.shape```\n",
    "\n",
    "**Second**, given that there are no weight parameters to these operations holds two implications:\n",
    "\n",
    "i) from a backward perspective, these operations are only intermediate variables and \n",
    "\n",
    "ii) we can just apply the derivative of the equation to each input such as shown below\n",
    "\n",
    "$$z = \\sigma(y)=\\sigma(x_ow_0+x_1w_1+x_2w_2+x_3w_3) = \\sigma(x_ow_0)+\\sigma(x_1w_1)+\\sigma(x_2w_2)+\\sigma(x_3w_3)$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial y}(x_ow_0+x_1w_1+x_2w_2+x_3w_3) = \\frac{\\partial z}{\\partial y}(x_ow_0)+\\frac{\\partial z}{\\partial y}(x_1w_1)+\\frac{\\partial z}{\\partial y}(x_2w_2)+\\frac{\\partial z}{\\partial y}(x_3w_3)$$\n",
    "\n",
    "Now that we generally understand how to implement non-linear operations, it begs to ask, **what are some common non-linear operations?**\n",
    "\n",
    "##### What are some Common non-linear operations?\n",
    "\n",
    "Well, lets break this down bit-by-bit by first showing a summary:\n",
    "\n",
    "* ReLU\n",
    "* Sigmoid\n",
    "* Tanh\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU\n",
    "ReLU is a piece-wise linear, vector valued function that adds non-linearity to our model. The effects that this simple piece-wise function has had on the DL sphere have been astonishing. \n",
    "\n",
    "The ReLU's forward and backward pass can both be seen as \"gates\" that either inhibit or advance the flow of either operations. \n",
    "\n",
    "During the forward pass, ReLU either retains the original content of the input if its greater than zero or else, turns it to zero.\n",
    "\n",
    "```python\n",
    "[x if x > 0 else 0 for x in input]\n",
    "```\n",
    "\n",
    "For the inputs that were \"cut\" to zero, its gradients are turned to zero while the rest of the values become 1. Hence, and given that ReLU is an intermediate operation, ReLU either restricts values of the incoming gradients or lets them \"flow\". \n",
    "\n",
    "Such simple conditions make ReLU a \"lightweight\" operation as it does not take much to compute its forward and backward method\n",
    "\n",
    "Such properties, and its surprising effectiveness to model non-linearity, have made ReLU a very popular choice of option for most DL architectures.\n",
    "\n",
    "Let us model this process in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1198,  0.1402],\n",
       "        [ 0.6488, -0.8397]], device='cuda:0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.randn((2,2)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom ReLU function \n",
    "# Remember that:\n",
    "# input.shape == out.shape == incoming_gradient.shape\n",
    "\n",
    "class ReLU_layer(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self, input):\n",
    "        # save input for backward() pass \n",
    "        self.save_for_backward(input) # wraps in a tuple structure\n",
    "        activated_input = torch.clamp(input, min = 0)\n",
    "        return activated_input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, output_grad_wrt_loss):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the \n",
    "        gradient of the loss with respect to our f(x) output, \n",
    "        and we now need to compute the gradient of the loss\n",
    "        wrt the input.\n",
    "        \"\"\"\n",
    "        # keep in mind that the gradient of ReLU is binary = {0,1}\n",
    "        # hence, we will either keep the element of the output_grad_wrt_loss\n",
    "        # or turn it to zero\n",
    "        input, = self.saved_tensors\n",
    "        input_grad_wrt_loss = output_grad_wrt_loss.clone()\n",
    "        input_grad_wrt_loss[input < 0] = 0\n",
    "        return input_grad_wrt_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap ReLU_layer function in nn.module\n",
    "class ReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = ReLU_layer.apply(input)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2316]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test function with linear + relu layer\n",
    "dummy_input= torch.ones((1,2)) # input \n",
    "\n",
    "# forward pass\n",
    "linear = nn.Linear(2,3)\n",
    "relu = ReLU()\n",
    "linear2 = nn.Linear(3,1)\n",
    "\n",
    "output1 = linear(dummy_input)\n",
    "output2 = relu(output1)\n",
    "output3 = linear2(output2)\n",
    "output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "output3.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1558, 0.1558],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check computed gradients of 1st linear layaer\n",
    "list(linear.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have validated our operation, let's us apply it to a simple neural network model that learns to map inputs from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_units = 128, activation = ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        # fully-connected layers\n",
    "        self.fc1 = nn.Linear(784,num_units)\n",
    "        self.fc2 = nn.Linear(num_units , num_units//2)\n",
    "        self.fc3 = nn.Linear(num_units // 2, 10)\n",
    "        \n",
    "        # init activation\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # 1st layer\n",
    "        output = self.activation(self.fc1(x))\n",
    "        \n",
    "        # 2nd layer\n",
    "        output = self.activation(self.fc2(output))\n",
    "        \n",
    "        # 3rd layer\n",
    "        output = self.fc3(output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: C:\\Users\\erick\\PycharmProjects\\untitled\\3D_2D_GAN\\MNIST_experimentation\n",
       "    Split: Train"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data and feed to DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root = r'C:\\Users\\erick\\PycharmProjects\\untitled\\3D_2D_GAN\\MNIST_experimentation'\n",
    "mnist = torchvision.datasets.MNIST(root = root, \n",
    "                                      train = True, \n",
    "                                      download = False, \n",
    "                                       transform = transforms.Normalize(.5,.5)\n",
    "                                  )\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize data to mean = 0, std = 1 and extract labels\n",
    "data = mnist.data.view(60000,-1).float()\n",
    "X = (data - data.mean()) / data.std()\n",
    "y = mnist.targets \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "from torch import optim\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    NeuralNet,\n",
    "    max_epochs = 25,\n",
    "    batch_size = 128,\n",
    "    lr = .01,\n",
    "    criterion = nn.CrossEntropyLoss,\n",
    "    optimizer = optim.SGD,\n",
    "    optimizer__momentum = .95,\n",
    "    device = 'cuda',\n",
    "    iterator_train__pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.4752       0.9411        0.1882  7.4694\n",
      "      2        0.1566       0.9623        0.1273  7.3764\n",
      "      3        0.1014       0.9664        0.1134  7.2953\n",
      "      4        0.0727       0.9697        0.1035  7.2914\n",
      "      5        0.0563       0.9741        0.0931  7.3200\n",
      "      6        0.0430       0.9713        0.1002  7.1467\n",
      "      7        0.0346       0.9708        0.1064  7.6154\n",
      "      8        0.0283       0.9693        0.1277  7.5981\n",
      "      9        0.0237       0.9698        0.1248  7.4488\n",
      "     10        0.0203       0.9738        0.1124  7.5092\n",
      "     11        0.0176       0.9714        0.1223  7.5436\n",
      "     12        0.0132       0.9747        0.1126  7.5255\n",
      "     13        0.0106       0.9745        0.1163  7.4018\n",
      "     14        0.0112       0.9744        0.1171  7.5277\n",
      "     15        0.0095       0.9752        0.1204  7.4607\n",
      "     16        0.0076       0.9763        0.1191  7.5565\n",
      "     17        0.0061       0.9756        0.1225  7.7586\n",
      "     18        0.0043       0.9770        0.1181  7.7375\n",
      "     19        0.0030       0.9763        0.1257  7.5261\n",
      "     20        0.0023       0.9768        0.1217  7.5623\n",
      "     21        0.0010       0.9792        0.1119  7.6008\n",
      "     22        0.0006       0.9797        0.1109  7.2899\n",
      "     23        0.0005       0.9803        0.1115  7.6545\n",
      "     24        0.0004       0.9800        0.1122  7.4768\n",
      "     25        0.0003       0.9801        0.1129  7.9523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=NeuralNet(\n",
       "    (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (activation): ReLU()\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batches</th>\n",
       "      <th>dur</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_batch_count</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_loss_best</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>valid_acc_best</th>\n",
       "      <th>valid_batch_count</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_loss_best</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'train_loss': 2.341770887374878, 'train_batc...</td>\n",
       "      <td>7.469449</td>\n",
       "      <td>1</td>\n",
       "      <td>375</td>\n",
       "      <td>0.475174</td>\n",
       "      <td>True</td>\n",
       "      <td>0.941108</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.188230</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'train_loss': 0.11716476827859879, 'train_ba...</td>\n",
       "      <td>7.376420</td>\n",
       "      <td>2</td>\n",
       "      <td>375</td>\n",
       "      <td>0.156564</td>\n",
       "      <td>True</td>\n",
       "      <td>0.962349</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.127276</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'train_loss': 0.07833997160196304, 'train_ba...</td>\n",
       "      <td>7.295328</td>\n",
       "      <td>3</td>\n",
       "      <td>375</td>\n",
       "      <td>0.101438</td>\n",
       "      <td>True</td>\n",
       "      <td>0.966431</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.113426</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'train_loss': 0.05629928037524223, 'train_ba...</td>\n",
       "      <td>7.291368</td>\n",
       "      <td>4</td>\n",
       "      <td>375</td>\n",
       "      <td>0.072656</td>\n",
       "      <td>True</td>\n",
       "      <td>0.969679</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.103544</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'train_loss': 0.039215993136167526, 'train_b...</td>\n",
       "      <td>7.319993</td>\n",
       "      <td>5</td>\n",
       "      <td>375</td>\n",
       "      <td>0.056315</td>\n",
       "      <td>True</td>\n",
       "      <td>0.974094</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.093118</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'train_loss': 0.01480671763420105, 'train_ba...</td>\n",
       "      <td>7.146651</td>\n",
       "      <td>6</td>\n",
       "      <td>375</td>\n",
       "      <td>0.043030</td>\n",
       "      <td>True</td>\n",
       "      <td>0.971345</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.100192</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'train_loss': 0.014157209545373917, 'train_b...</td>\n",
       "      <td>7.615449</td>\n",
       "      <td>7</td>\n",
       "      <td>375</td>\n",
       "      <td>0.034617</td>\n",
       "      <td>True</td>\n",
       "      <td>0.970762</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.106429</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[{'train_loss': 0.028001882135868073, 'train_b...</td>\n",
       "      <td>7.598089</td>\n",
       "      <td>8</td>\n",
       "      <td>375</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>True</td>\n",
       "      <td>0.969263</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.127717</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[{'train_loss': 0.017053615301847458, 'train_b...</td>\n",
       "      <td>7.448816</td>\n",
       "      <td>9</td>\n",
       "      <td>375</td>\n",
       "      <td>0.023698</td>\n",
       "      <td>True</td>\n",
       "      <td>0.969763</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.124840</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[{'train_loss': 0.006474234163761139, 'train_b...</td>\n",
       "      <td>7.509183</td>\n",
       "      <td>10</td>\n",
       "      <td>375</td>\n",
       "      <td>0.020254</td>\n",
       "      <td>True</td>\n",
       "      <td>0.973844</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.112428</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[{'train_loss': 0.008814588189125061, 'train_b...</td>\n",
       "      <td>7.543603</td>\n",
       "      <td>11</td>\n",
       "      <td>375</td>\n",
       "      <td>0.017601</td>\n",
       "      <td>True</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.122295</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[{'train_loss': 0.02929287776350975, 'train_ba...</td>\n",
       "      <td>7.525495</td>\n",
       "      <td>12</td>\n",
       "      <td>375</td>\n",
       "      <td>0.013164</td>\n",
       "      <td>True</td>\n",
       "      <td>0.974677</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.112637</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[{'train_loss': 0.010828588157892227, 'train_b...</td>\n",
       "      <td>7.401756</td>\n",
       "      <td>13</td>\n",
       "      <td>375</td>\n",
       "      <td>0.010614</td>\n",
       "      <td>True</td>\n",
       "      <td>0.974511</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.116338</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[{'train_loss': 0.005119025707244873, 'train_b...</td>\n",
       "      <td>7.527719</td>\n",
       "      <td>14</td>\n",
       "      <td>375</td>\n",
       "      <td>0.011242</td>\n",
       "      <td>False</td>\n",
       "      <td>0.974427</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.117143</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[{'train_loss': 0.00426182895898819, 'train_ba...</td>\n",
       "      <td>7.460663</td>\n",
       "      <td>15</td>\n",
       "      <td>375</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>True</td>\n",
       "      <td>0.975177</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.120371</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[{'train_loss': 0.0053856708109378815, 'train_...</td>\n",
       "      <td>7.556471</td>\n",
       "      <td>16</td>\n",
       "      <td>375</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>True</td>\n",
       "      <td>0.976260</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.119120</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[{'train_loss': 0.0021658167243003845, 'train_...</td>\n",
       "      <td>7.758590</td>\n",
       "      <td>17</td>\n",
       "      <td>375</td>\n",
       "      <td>0.006060</td>\n",
       "      <td>True</td>\n",
       "      <td>0.975594</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.122540</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[{'train_loss': 0.006312757730484009, 'train_b...</td>\n",
       "      <td>7.737470</td>\n",
       "      <td>18</td>\n",
       "      <td>375</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>True</td>\n",
       "      <td>0.977010</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.118147</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[{'train_loss': 0.0006145685911178589, 'train_...</td>\n",
       "      <td>7.526064</td>\n",
       "      <td>19</td>\n",
       "      <td>375</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>True</td>\n",
       "      <td>0.976343</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.125696</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[{'train_loss': 0.0022733211517333984, 'train_...</td>\n",
       "      <td>7.562322</td>\n",
       "      <td>20</td>\n",
       "      <td>375</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>True</td>\n",
       "      <td>0.976760</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.121666</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[{'train_loss': 0.0009470283985137939, 'train_...</td>\n",
       "      <td>7.600832</td>\n",
       "      <td>21</td>\n",
       "      <td>375</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>True</td>\n",
       "      <td>0.979175</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.111854</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[{'train_loss': 0.0005830973386764526, 'train_...</td>\n",
       "      <td>7.289893</td>\n",
       "      <td>22</td>\n",
       "      <td>375</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>True</td>\n",
       "      <td>0.979675</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.110884</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[{'train_loss': 0.0003417953848838806, 'train_...</td>\n",
       "      <td>7.654543</td>\n",
       "      <td>23</td>\n",
       "      <td>375</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>True</td>\n",
       "      <td>0.980258</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "      <td>0.111454</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[{'train_loss': 0.00024148821830749512, 'train...</td>\n",
       "      <td>7.476789</td>\n",
       "      <td>24</td>\n",
       "      <td>375</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>True</td>\n",
       "      <td>0.980008</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.112246</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[{'train_loss': 0.0002155303955078125, 'train_...</td>\n",
       "      <td>7.952273</td>\n",
       "      <td>25</td>\n",
       "      <td>375</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>True</td>\n",
       "      <td>0.980092</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>0.112883</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              batches       dur  epoch  \\\n",
       "0   [{'train_loss': 2.341770887374878, 'train_batc...  7.469449      1   \n",
       "1   [{'train_loss': 0.11716476827859879, 'train_ba...  7.376420      2   \n",
       "2   [{'train_loss': 0.07833997160196304, 'train_ba...  7.295328      3   \n",
       "3   [{'train_loss': 0.05629928037524223, 'train_ba...  7.291368      4   \n",
       "4   [{'train_loss': 0.039215993136167526, 'train_b...  7.319993      5   \n",
       "5   [{'train_loss': 0.01480671763420105, 'train_ba...  7.146651      6   \n",
       "6   [{'train_loss': 0.014157209545373917, 'train_b...  7.615449      7   \n",
       "7   [{'train_loss': 0.028001882135868073, 'train_b...  7.598089      8   \n",
       "8   [{'train_loss': 0.017053615301847458, 'train_b...  7.448816      9   \n",
       "9   [{'train_loss': 0.006474234163761139, 'train_b...  7.509183     10   \n",
       "10  [{'train_loss': 0.008814588189125061, 'train_b...  7.543603     11   \n",
       "11  [{'train_loss': 0.02929287776350975, 'train_ba...  7.525495     12   \n",
       "12  [{'train_loss': 0.010828588157892227, 'train_b...  7.401756     13   \n",
       "13  [{'train_loss': 0.005119025707244873, 'train_b...  7.527719     14   \n",
       "14  [{'train_loss': 0.00426182895898819, 'train_ba...  7.460663     15   \n",
       "15  [{'train_loss': 0.0053856708109378815, 'train_...  7.556471     16   \n",
       "16  [{'train_loss': 0.0021658167243003845, 'train_...  7.758590     17   \n",
       "17  [{'train_loss': 0.006312757730484009, 'train_b...  7.737470     18   \n",
       "18  [{'train_loss': 0.0006145685911178589, 'train_...  7.526064     19   \n",
       "19  [{'train_loss': 0.0022733211517333984, 'train_...  7.562322     20   \n",
       "20  [{'train_loss': 0.0009470283985137939, 'train_...  7.600832     21   \n",
       "21  [{'train_loss': 0.0005830973386764526, 'train_...  7.289893     22   \n",
       "22  [{'train_loss': 0.0003417953848838806, 'train_...  7.654543     23   \n",
       "23  [{'train_loss': 0.00024148821830749512, 'train...  7.476789     24   \n",
       "24  [{'train_loss': 0.0002155303955078125, 'train_...  7.952273     25   \n",
       "\n",
       "    train_batch_count  train_loss  train_loss_best  valid_acc  valid_acc_best  \\\n",
       "0                 375    0.475174             True   0.941108            True   \n",
       "1                 375    0.156564             True   0.962349            True   \n",
       "2                 375    0.101438             True   0.966431            True   \n",
       "3                 375    0.072656             True   0.969679            True   \n",
       "4                 375    0.056315             True   0.974094            True   \n",
       "5                 375    0.043030             True   0.971345           False   \n",
       "6                 375    0.034617             True   0.970762           False   \n",
       "7                 375    0.028333             True   0.969263           False   \n",
       "8                 375    0.023698             True   0.969763           False   \n",
       "9                 375    0.020254             True   0.973844           False   \n",
       "10                375    0.017601             True   0.971429           False   \n",
       "11                375    0.013164             True   0.974677            True   \n",
       "12                375    0.010614             True   0.974511           False   \n",
       "13                375    0.011242            False   0.974427           False   \n",
       "14                375    0.009507             True   0.975177            True   \n",
       "15                375    0.007564             True   0.976260            True   \n",
       "16                375    0.006060             True   0.975594           False   \n",
       "17                375    0.004327             True   0.977010            True   \n",
       "18                375    0.002974             True   0.976343           False   \n",
       "19                375    0.002272             True   0.976760           False   \n",
       "20                375    0.001029             True   0.979175            True   \n",
       "21                375    0.000626             True   0.979675            True   \n",
       "22                375    0.000471             True   0.980258            True   \n",
       "23                375    0.000392             True   0.980008           False   \n",
       "24                375    0.000344             True   0.980092           False   \n",
       "\n",
       "    valid_batch_count  valid_loss  valid_loss_best  \n",
       "0                  94    0.188230             True  \n",
       "1                  94    0.127276             True  \n",
       "2                  94    0.113426             True  \n",
       "3                  94    0.103544             True  \n",
       "4                  94    0.093118             True  \n",
       "5                  94    0.100192            False  \n",
       "6                  94    0.106429            False  \n",
       "7                  94    0.127717            False  \n",
       "8                  94    0.124840            False  \n",
       "9                  94    0.112428            False  \n",
       "10                 94    0.122295            False  \n",
       "11                 94    0.112637            False  \n",
       "12                 94    0.116338            False  \n",
       "13                 94    0.117143            False  \n",
       "14                 94    0.120371            False  \n",
       "15                 94    0.119120            False  \n",
       "16                 94    0.122540            False  \n",
       "17                 94    0.118147            False  \n",
       "18                 94    0.125696            False  \n",
       "19                 94    0.121666            False  \n",
       "20                 94    0.111854            False  \n",
       "21                 94    0.110884            False  \n",
       "22                 94    0.111454            False  \n",
       "23                 94    0.112246            False  \n",
       "24                 94    0.112883            False  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract history\n",
    "import pandas as pd\n",
    "history = pd.DataFrame(net.history)\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_loss_best</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>valid_acc_best</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_loss_best</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.475174</td>\n",
       "      <td>True</td>\n",
       "      <td>0.941108</td>\n",
       "      <td>True</td>\n",
       "      <td>0.188230</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.156564</td>\n",
       "      <td>True</td>\n",
       "      <td>0.962349</td>\n",
       "      <td>True</td>\n",
       "      <td>0.127276</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.101438</td>\n",
       "      <td>True</td>\n",
       "      <td>0.966431</td>\n",
       "      <td>True</td>\n",
       "      <td>0.113426</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.072656</td>\n",
       "      <td>True</td>\n",
       "      <td>0.969679</td>\n",
       "      <td>True</td>\n",
       "      <td>0.103544</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.056315</td>\n",
       "      <td>True</td>\n",
       "      <td>0.974094</td>\n",
       "      <td>True</td>\n",
       "      <td>0.093118</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.043030</td>\n",
       "      <td>True</td>\n",
       "      <td>0.971345</td>\n",
       "      <td>False</td>\n",
       "      <td>0.100192</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.034617</td>\n",
       "      <td>True</td>\n",
       "      <td>0.970762</td>\n",
       "      <td>False</td>\n",
       "      <td>0.106429</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>True</td>\n",
       "      <td>0.969263</td>\n",
       "      <td>False</td>\n",
       "      <td>0.127717</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.023698</td>\n",
       "      <td>True</td>\n",
       "      <td>0.969763</td>\n",
       "      <td>False</td>\n",
       "      <td>0.124840</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.020254</td>\n",
       "      <td>True</td>\n",
       "      <td>0.973844</td>\n",
       "      <td>False</td>\n",
       "      <td>0.112428</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.017601</td>\n",
       "      <td>True</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>False</td>\n",
       "      <td>0.122295</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.013164</td>\n",
       "      <td>True</td>\n",
       "      <td>0.974677</td>\n",
       "      <td>True</td>\n",
       "      <td>0.112637</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.010614</td>\n",
       "      <td>True</td>\n",
       "      <td>0.974511</td>\n",
       "      <td>False</td>\n",
       "      <td>0.116338</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.011242</td>\n",
       "      <td>False</td>\n",
       "      <td>0.974427</td>\n",
       "      <td>False</td>\n",
       "      <td>0.117143</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>True</td>\n",
       "      <td>0.975177</td>\n",
       "      <td>True</td>\n",
       "      <td>0.120371</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>True</td>\n",
       "      <td>0.976260</td>\n",
       "      <td>True</td>\n",
       "      <td>0.119120</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.006060</td>\n",
       "      <td>True</td>\n",
       "      <td>0.975594</td>\n",
       "      <td>False</td>\n",
       "      <td>0.122540</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>True</td>\n",
       "      <td>0.977010</td>\n",
       "      <td>True</td>\n",
       "      <td>0.118147</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>True</td>\n",
       "      <td>0.976343</td>\n",
       "      <td>False</td>\n",
       "      <td>0.125696</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>True</td>\n",
       "      <td>0.976760</td>\n",
       "      <td>False</td>\n",
       "      <td>0.121666</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>True</td>\n",
       "      <td>0.979175</td>\n",
       "      <td>True</td>\n",
       "      <td>0.111854</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>True</td>\n",
       "      <td>0.979675</td>\n",
       "      <td>True</td>\n",
       "      <td>0.110884</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>True</td>\n",
       "      <td>0.980258</td>\n",
       "      <td>True</td>\n",
       "      <td>0.111454</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>True</td>\n",
       "      <td>0.980008</td>\n",
       "      <td>False</td>\n",
       "      <td>0.112246</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>True</td>\n",
       "      <td>0.980092</td>\n",
       "      <td>False</td>\n",
       "      <td>0.112883</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  train_loss_best  valid_acc  valid_acc_best  valid_loss  \\\n",
       "0       1    0.475174             True   0.941108            True    0.188230   \n",
       "1       2    0.156564             True   0.962349            True    0.127276   \n",
       "2       3    0.101438             True   0.966431            True    0.113426   \n",
       "3       4    0.072656             True   0.969679            True    0.103544   \n",
       "4       5    0.056315             True   0.974094            True    0.093118   \n",
       "5       6    0.043030             True   0.971345           False    0.100192   \n",
       "6       7    0.034617             True   0.970762           False    0.106429   \n",
       "7       8    0.028333             True   0.969263           False    0.127717   \n",
       "8       9    0.023698             True   0.969763           False    0.124840   \n",
       "9      10    0.020254             True   0.973844           False    0.112428   \n",
       "10     11    0.017601             True   0.971429           False    0.122295   \n",
       "11     12    0.013164             True   0.974677            True    0.112637   \n",
       "12     13    0.010614             True   0.974511           False    0.116338   \n",
       "13     14    0.011242            False   0.974427           False    0.117143   \n",
       "14     15    0.009507             True   0.975177            True    0.120371   \n",
       "15     16    0.007564             True   0.976260            True    0.119120   \n",
       "16     17    0.006060             True   0.975594           False    0.122540   \n",
       "17     18    0.004327             True   0.977010            True    0.118147   \n",
       "18     19    0.002974             True   0.976343           False    0.125696   \n",
       "19     20    0.002272             True   0.976760           False    0.121666   \n",
       "20     21    0.001029             True   0.979175            True    0.111854   \n",
       "21     22    0.000626             True   0.979675            True    0.110884   \n",
       "22     23    0.000471             True   0.980258            True    0.111454   \n",
       "23     24    0.000392             True   0.980008           False    0.112246   \n",
       "24     25    0.000344             True   0.980092           False    0.112883   \n",
       "\n",
       "    valid_loss_best  \n",
       "0              True  \n",
       "1              True  \n",
       "2              True  \n",
       "3              True  \n",
       "4              True  \n",
       "5             False  \n",
       "6             False  \n",
       "7             False  \n",
       "8             False  \n",
       "9             False  \n",
       "10            False  \n",
       "11            False  \n",
       "12            False  \n",
       "13            False  \n",
       "14            False  \n",
       "15            False  \n",
       "16            False  \n",
       "17            False  \n",
       "18            False  \n",
       "19            False  \n",
       "20            False  \n",
       "21            False  \n",
       "22            False  \n",
       "23            False  \n",
       "24            False  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract only epoch, train_loss, train_loss_best, valid_acc, valid_acc_best, valid_loss, and valid_loss_best\n",
    "df = history.iloc[:, [2,4,5,6,7,9,10]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'train_loss': 0.47517417672773565,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9411078717201167,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.18822958134875006,\n",
       "  'valid_loss_best': True},\n",
       " {'epoch': 2,\n",
       "  'train_loss': 0.15656359225421515,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9623490212411495,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.12727606778018727,\n",
       "  'valid_loss_best': True},\n",
       " {'epoch': 3,\n",
       "  'train_loss': 0.10143763750204755,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9664306538942108,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.11342565075016081,\n",
       "  'valid_loss_best': True},\n",
       " {'epoch': 4,\n",
       "  'train_loss': 0.07265592684428113,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9696793002915451,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.10354374069092126,\n",
       "  'valid_loss_best': True},\n",
       " {'epoch': 5,\n",
       "  'train_loss': 0.056315004594910456,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9740941274468972,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.09311790618299197,\n",
       "  'valid_loss_best': True},\n",
       " {'epoch': 6,\n",
       "  'train_loss': 0.043029631272071676,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9713452728029988,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.10019202414513727,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 7,\n",
       "  'train_loss': 0.03461681064876098,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.97076218242399,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.10642864949278213,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 8,\n",
       "  'train_loss': 0.0283333105644915,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9692628071636818,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.12771687501547288,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 9,\n",
       "  'train_loss': 0.023698081428066543,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9697625989171179,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.1248397599590763,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 10,\n",
       "  'train_loss': 0.020254216069249462,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9738442315701791,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.11242812531148827,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 11,\n",
       "  'train_loss': 0.017600739855718608,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9714285714285714,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.12229490509968109,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 12,\n",
       "  'train_loss': 0.013163874244699876,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9746772178259059,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.112637081089236,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 13,\n",
       "  'train_loss': 0.010614331251107128,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9745106205747606,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.11633807773337618,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 14,\n",
       "  'train_loss': 0.011242055681822561,\n",
       "  'train_loss_best': False,\n",
       "  'valid_acc': 0.9744273219491878,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.11714292346270617,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 15,\n",
       "  'train_loss': 0.009507204108821164,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.975177009579342,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.12037123334995536,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 16,\n",
       "  'train_loss': 0.0075637328857890715,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9762598917117867,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.11912028238981007,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 17,\n",
       "  'train_loss': 0.006059871142913878,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9755935027072054,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.12253995682580279,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 18,\n",
       "  'train_loss': 0.0043265555326195545,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9770095793419409,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.11814686613438999,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 19,\n",
       "  'train_loss': 0.0029741345549760814,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9763431903373594,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.12569615992506822,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 20,\n",
       "  'train_loss': 0.0022717719697760708,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9767596834652228,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.12166584508647922,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 21,\n",
       "  'train_loss': 0.0010286334160211211,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9791753436068305,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.11185363285904475,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 22,\n",
       "  'train_loss': 0.0006259158970377714,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9796751353602665,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.11088434621344155,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 23,\n",
       "  'train_loss': 0.0004706797544968735,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9802582257392753,\n",
       "  'valid_acc_best': True,\n",
       "  'valid_loss': 0.11145350747694824,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 24,\n",
       "  'train_loss': 0.0003918637830194725,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9800083298625573,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.1122461453165513,\n",
       "  'valid_loss_best': False},\n",
       " {'epoch': 25,\n",
       "  'train_loss': 0.00034387034910720053,\n",
       "  'train_loss_best': True,\n",
       "  'valid_acc': 0.9800916284881299,\n",
       "  'valid_acc_best': False,\n",
       "  'valid_loss': 0.112882753556865,\n",
       "  'valid_loss_best': False}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn above data into a list of dictionary values so that we can feed\n",
    "# to HiPlot\n",
    "results = []\n",
    "for row in df.iterrows():\n",
    "    results.append(row[1].to_dict())\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"wrap_html_2ee62e\"><!DOCTYPE html>\n",
       "<html>\n",
       "  <head>\n",
       "    <meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/>\n",
       "    <title>HiPlot</title>\n",
       "    <link rel=\"icon\" href=\"static/icon.png\" />\n",
       "  </head>\n",
       "  <body style=\"margin:0px\">\n",
       "    <div id=\"hiplot_c1b8f90fb155425b9c75f1661b7a2c3d\" style=\"background-color: white\"><div style=\"text-align: center\">Loading HiPlot...</div>\n",
       "      <noscript>\n",
       "        HiPlot needs JavaScript to run\n",
       "      </noscript>\n",
       "    </div>\n",
       "\n",
       "    <script  type=\"text/javascript\" ></script>\n",
       "    <script type=\"text/javascript\">\n",
       "    function _hiplot_setup() {\n",
       "      if (typeof hiplot_setup == \"undefined\") {\n",
       "        console.info(\"waiting for HiPlot bundle to be loaded...\");\n",
       "        setTimeout(_hiplot_setup, 1000);\n",
       "        return;\n",
       "      }\n",
       "      var options = {\n",
       "        is_webserver: true,\n",
       "      };\n",
       "      /*ON_LOAD_SCRIPT_INJECT*/\n",
       "const comm_id = \"comm_d17d9b\";\n",
       "try {\n",
       "    console.log(\"Setting up communication channel with Jupyter: \", comm_id);\n",
       "    var comm = Jupyter.notebook.kernel.comm_manager.new_comm(comm_id, {'type': 'hello'});\n",
       "    Object.assign(options, {\"comm\": comm});\n",
       "}\n",
       "catch(err) {\n",
       "    console.warn('Unable to create Javascript <-> Python communication channel' +\n",
       "        ' (are you in a Jupyter notebook? Jupyter labs is *not* supported!)');\n",
       "}\n",
       "        \n",
       "        Object.assign(options, eval('(' + \"{\\\"is_webserver\\\": false, \\\"experiment\\\": {\\\"datapoints\\\": [{\\\"uid\\\": \\\"0\\\", \\\"values\\\": {\\\"epoch\\\": 1, \\\"train_loss\\\": 0.47517417672773565, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9411078717201167, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.18822958134875006, \\\"valid_loss_best\\\": true}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"1\\\", \\\"values\\\": {\\\"epoch\\\": 2, \\\"train_loss\\\": 0.15656359225421515, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9623490212411495, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.12727606778018727, \\\"valid_loss_best\\\": true}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"2\\\", \\\"values\\\": {\\\"epoch\\\": 3, \\\"train_loss\\\": 0.10143763750204755, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9664306538942108, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.11342565075016081, \\\"valid_loss_best\\\": true}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"3\\\", \\\"values\\\": {\\\"epoch\\\": 4, \\\"train_loss\\\": 0.07265592684428113, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9696793002915451, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.10354374069092126, \\\"valid_loss_best\\\": true}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"4\\\", \\\"values\\\": {\\\"epoch\\\": 5, \\\"train_loss\\\": 0.056315004594910456, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9740941274468972, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.09311790618299197, \\\"valid_loss_best\\\": true}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"5\\\", \\\"values\\\": {\\\"epoch\\\": 6, \\\"train_loss\\\": 0.043029631272071676, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9713452728029988, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.10019202414513727, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"6\\\", \\\"values\\\": {\\\"epoch\\\": 7, \\\"train_loss\\\": 0.03461681064876098, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.97076218242399, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.10642864949278213, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"7\\\", \\\"values\\\": {\\\"epoch\\\": 8, \\\"train_loss\\\": 0.0283333105644915, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9692628071636818, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.12771687501547288, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"8\\\", \\\"values\\\": {\\\"epoch\\\": 9, \\\"train_loss\\\": 0.023698081428066543, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9697625989171179, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.1248397599590763, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"9\\\", \\\"values\\\": {\\\"epoch\\\": 10, \\\"train_loss\\\": 0.020254216069249462, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9738442315701791, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.11242812531148827, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"10\\\", \\\"values\\\": {\\\"epoch\\\": 11, \\\"train_loss\\\": 0.017600739855718608, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9714285714285714, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.12229490509968109, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"11\\\", \\\"values\\\": {\\\"epoch\\\": 12, \\\"train_loss\\\": 0.013163874244699876, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9746772178259059, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.112637081089236, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"12\\\", \\\"values\\\": {\\\"epoch\\\": 13, \\\"train_loss\\\": 0.010614331251107128, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9745106205747606, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.11633807773337618, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"13\\\", \\\"values\\\": {\\\"epoch\\\": 14, \\\"train_loss\\\": 0.011242055681822561, \\\"train_loss_best\\\": false, \\\"valid_acc\\\": 0.9744273219491878, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.11714292346270617, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"14\\\", \\\"values\\\": {\\\"epoch\\\": 15, \\\"train_loss\\\": 0.009507204108821164, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.975177009579342, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.12037123334995536, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"15\\\", \\\"values\\\": {\\\"epoch\\\": 16, \\\"train_loss\\\": 0.0075637328857890715, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9762598917117867, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.11912028238981007, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"16\\\", \\\"values\\\": {\\\"epoch\\\": 17, \\\"train_loss\\\": 0.006059871142913878, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9755935027072054, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.12253995682580279, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"17\\\", \\\"values\\\": {\\\"epoch\\\": 18, \\\"train_loss\\\": 0.0043265555326195545, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9770095793419409, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.11814686613438999, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"18\\\", \\\"values\\\": {\\\"epoch\\\": 19, \\\"train_loss\\\": 0.0029741345549760814, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9763431903373594, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.12569615992506822, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"19\\\", \\\"values\\\": {\\\"epoch\\\": 20, \\\"train_loss\\\": 0.0022717719697760708, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9767596834652228, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.12166584508647922, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"20\\\", \\\"values\\\": {\\\"epoch\\\": 21, \\\"train_loss\\\": 0.0010286334160211211, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9791753436068305, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.11185363285904475, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"21\\\", \\\"values\\\": {\\\"epoch\\\": 22, \\\"train_loss\\\": 0.0006259158970377714, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9796751353602665, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.11088434621344155, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"22\\\", \\\"values\\\": {\\\"epoch\\\": 23, \\\"train_loss\\\": 0.0004706797544968735, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9802582257392753, \\\"valid_acc_best\\\": true, \\\"valid_loss\\\": 0.11145350747694824, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"23\\\", \\\"values\\\": {\\\"epoch\\\": 24, \\\"train_loss\\\": 0.0003918637830194725, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9800083298625573, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.1122461453165513, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"24\\\", \\\"values\\\": {\\\"epoch\\\": 25, \\\"train_loss\\\": 0.00034387034910720053, \\\"train_loss_best\\\": true, \\\"valid_acc\\\": 0.9800916284881299, \\\"valid_acc_best\\\": false, \\\"valid_loss\\\": 0.112882753556865, \\\"valid_loss_best\\\": false}, \\\"from_uid\\\": null}], \\\"parameters_definition\\\": {}, \\\"_displays\\\": {\\\"PARALLEL_PLOT\\\": {}, \\\"TABLE\\\": {}, \\\"XY\\\": {}}}, \\\"persistent_state\\\": null}\" + ')'));\n",
       "        \n",
       "      var hiplot_instance = hiplot_setup(document.getElementById(\"hiplot_c1b8f90fb155425b9c75f1661b7a2c3d\"), options);\n",
       "      /*AFTER_SETUP_SCRIPT_INJECT*/\n",
       "    }\n",
       "    if (document.readyState == \"complete\") {\n",
       "      _hiplot_setup();\n",
       "    }\n",
       "    else {\n",
       "      document.addEventListener(\"DOMContentLoaded\", _hiplot_setup);\n",
       "    }\n",
       "    </script>\n",
       "  </body>\n",
       "</html></div>\n",
       "<script type=\"text/javascript\">\n",
       "(function() {\n",
       "    const elem = document.getElementById(\"wrap_html_2ee62e\");\n",
       "    elem.style.width = \"100vw\";\n",
       "    const removeElems = elem.parentElement.parentElement.getElementsByClassName(\"prompt\");\n",
       "    for (var i = 0; i < removeElems.length; ++i) {\n",
       "        removeElems[i].remove();\n",
       "    }\n",
       "    elem.parentElement.style.overflowX = \"visible\";\n",
       "\n",
       "    const scale_to_100pct_screen = function() {\n",
       "        elem.style.marginLeft = '0px';\n",
       "        elem.style.marginLeft = (- elem.getBoundingClientRect().x) + 'px';\n",
       "    };\n",
       "    window.addEventListener('resize', function() {\n",
       "        scale_to_100pct_screen();\n",
       "    });\n",
       "    scale_to_100pct_screen();\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<hiplot.ipython.IPythonExperimentDisplayed at 0x2c19b5b1278>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(results, 'relu_history.pt')\n",
    "import hiplot as hip\n",
    "# hip.Experiment.from_iterable(results).display() for a smaller visualization\n",
    "hip.Experiment.from_iterable(results).display(force_full_width = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous insights that we can gather from the above graph:\n",
    "\n",
    "* As number of epochs increase, train loss begins to decrease lower than its previous epoch (with the exception of epoch 14)\n",
    "* As train loss begins to decrease, validation accuracy begins to climb\n",
    "* The higher the validation accuracy, the more of a chance it has to output the best validation accuracy\n",
    "* For validations with the best accuracy, the more of a chance there is to also attain the  best validation loss\n",
    "\n",
    "Such insights lets us know that the data may be overfitting once it reaches epochs greater than 15 as the model begins to slow down its learning and best validation accuracies begin to decrease.\n",
    "\n",
    "In the ML world, it is important to be able to present your findings in a meaningful way that enables us to compare our findings with other state-of-the-art models. \n",
    "\n",
    "For such reason, we will again implement our above model, however, this time we will be comparing the performances of different activation methods. \n",
    "\n",
    "We will be comparing the following:\n",
    "\n",
    "* ReLU\n",
    "* tanh\n",
    "* leakyReLU\n",
    "\n",
    "For concreteness, we will implement tanh and leakyReLU with PyTorch's inherent methods and its manual implementation may be left as an exercise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\n",
    "    'module__activation': [ReLU(), nn.Tanh(), nn.LeakyReLU()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will run a GridSearch of the above parameters on 3-folds\n",
    "gs = GridSearchCV(net, params, refit = False,cv = 3,scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer because the following parameters were re-set: momentum.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.6218       0.9190        0.2577  3.8483\n",
      "      2        0.1946       0.9483        0.1678  3.7291\n",
      "      3        0.1281       0.9563        0.1405  3.8284\n",
      "      4        0.0918       0.9608        0.1324  4.0050\n",
      "      5        0.0682       0.9628        0.1276  3.8801\n",
      "      6        0.0523       0.9626        0.1330  3.8397\n",
      "      7        0.0399       0.9655        0.1249  4.2604\n",
      "      8        0.0319       0.9654        0.1265  4.0021\n",
      "      9        0.0267       0.9659        0.1271  3.6260\n",
      "     10        0.0242       0.9639        0.1444  3.8109\n",
      "     11        0.0199       0.9625        0.1435  3.8044\n",
      "     12        0.0178       0.9646        0.1438  3.6571\n",
      "     13        0.0153       0.9689        0.1394  3.5908\n",
      "     14        0.0136       0.9695        0.1397  3.7299\n",
      "     15        0.0116       0.9675        0.1539  4.0582\n",
      "     16        0.0117       0.9670        0.1523  3.6810\n",
      "     17        0.0112       0.9589        0.1999  3.8220\n",
      "     18        0.0126       0.9681        0.1422  3.9593\n",
      "     19        0.0063       0.9706        0.1412  3.5138\n",
      "     20        0.0046       0.9698        0.1469  3.8316\n",
      "     21        0.0022       0.9731        0.1334  3.6366\n",
      "     22        0.0012       0.9743        0.1303  3.9471\n",
      "     23        0.0007       0.9746        0.1317  3.6123\n",
      "     24        0.0006       0.9749        0.1330  4.0988\n",
      "     25        0.0005       0.9750        0.1339  3.9482\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer because the following parameters were re-set: momentum.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.5870       0.9229        0.2452  3.7478\n",
      "      2        0.2027       0.9479        0.1612  3.6823\n",
      "      3        0.1347       0.9610        0.1298  3.6192\n",
      "      4        0.0936       0.9634        0.1189  3.6431\n",
      "      5        0.0700       0.9664        0.1102  4.0279\n",
      "      6        0.0556       0.9673        0.1143  3.9734\n",
      "      7        0.0453       0.9694        0.1100  3.6163\n",
      "      8        0.0366       0.9668        0.1217  3.7035\n",
      "      9        0.0310       0.9694        0.1238  3.7948\n",
      "     10        0.0273       0.9686        0.1242  3.8817\n",
      "     11        0.0221       0.9658        0.1315  3.7679\n",
      "     12        0.0196       0.9696        0.1279  3.9839\n",
      "     13        0.0161       0.9725        0.1164  3.7015\n",
      "     14        0.0143       0.9728        0.1199  3.9681\n",
      "     15        0.0110       0.9754        0.1133  3.6409\n",
      "     16        0.0084       0.9728        0.1201  3.6645\n",
      "     17        0.0071       0.9756        0.1145  3.8598\n",
      "     18        0.0049       0.9759        0.1170  3.8818\n",
      "     19        0.0034       0.9769        0.1102  3.8235\n",
      "     20        0.0021       0.9760        0.1205  3.7398\n",
      "     21        0.0013       0.9778        0.1114  3.7610\n",
      "     22        0.0009       0.9794        0.1066  3.3373\n",
      "     23        0.0006       0.9794        0.1081  2.1650\n",
      "     24        0.0005       0.9793        0.1095  2.1570\n",
      "     25        0.0004       0.9791        0.1104  2.1050\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer because the following parameters were re-set: momentum.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.5906       0.9305        0.2352  2.1590\n",
      "      2        0.1962       0.9507        0.1627  2.2660\n",
      "      3        0.1320       0.9604        0.1305  2.2340\n",
      "      4        0.0955       0.9649        0.1175  2.2780\n",
      "      5        0.0741       0.9683        0.1016  2.3420\n",
      "      6        0.0580       0.9711        0.0951  2.1950\n",
      "      7        0.0439       0.9714        0.0961  2.2560\n",
      "      8        0.0336       0.9711        0.0991  2.2660\n",
      "      9        0.0268       0.9726        0.1004  2.3130\n",
      "     10        0.0219       0.9725        0.1043  2.2750\n",
      "     11        0.0196       0.9736        0.1057  2.3170\n",
      "     12        0.0169       0.9738        0.1032  2.4230\n",
      "     13        0.0120       0.9730        0.1101  2.3510\n",
      "     14        0.0089       0.9740        0.1082  2.3490\n",
      "     15        0.0065       0.9715        0.1262  2.4380\n",
      "     16        0.0042       0.9763        0.1087  2.3150\n",
      "     17        0.0035       0.9761        0.1110  2.3440\n",
      "     18        0.0025       0.9770        0.1109  2.3100\n",
      "     19        0.0017       0.9774        0.1095  2.3330\n",
      "     20        0.0012       0.9773        0.1102  2.3330\n",
      "     21        0.0010       0.9773        0.1112  2.3270\n",
      "     22        0.0008       0.9774        0.1119  2.3650\n",
      "     23        0.0007       0.9774        0.1125  2.4260\n",
      "     24        0.0006       0.9771        0.1131  2.3610\n",
      "     25        0.0006       0.9771        0.1137  2.4420\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer because the following parameters were re-set: momentum.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.5449       0.9279        0.2478  2.0750\n",
      "      2        0.1996       0.9454        0.1828  2.1200\n",
      "      3        0.1341       0.9525        0.1545  2.1080\n",
      "      4        0.0970       0.9560        0.1405  2.1200\n",
      "      5        0.0724       0.9599        0.1302  2.0400\n",
      "      6        0.0544       0.9631        0.1234  2.0370\n",
      "      7        0.0405       0.9648        0.1188  2.1410\n",
      "      8        0.0302       0.9646        0.1188  2.0660\n",
      "      9        0.0227       0.9649        0.1204  2.0780\n",
      "     10        0.0173       0.9655        0.1197  2.1030\n",
      "     11        0.0135       0.9653        0.1205  2.0610\n",
      "     12        0.0109       0.9651        0.1208  2.0680\n",
      "     13        0.0090       0.9678        0.1171  2.1320\n",
      "     14        0.0079       0.9680        0.1178  2.0730\n",
      "     15        0.0070       0.9675        0.1189  2.0760\n",
      "     16        0.0060       0.9680        0.1203  2.0780\n",
      "     17        0.0051       0.9686        0.1193  2.0790\n",
      "     18        0.0044       0.9686        0.1201  2.0900\n",
      "     19        0.0038       0.9675        0.1223  2.0820\n",
      "     20        0.0034       0.9683        0.1236  2.0970\n",
      "     21        0.0033       0.9683        0.1285  2.1020\n",
      "     22        0.0026       0.9705        0.1175  2.0990\n",
      "     23        0.0020       0.9696        0.1180  2.2400\n",
      "     24        0.0018       0.9698        0.1183  3.2590\n",
      "     25        0.0017       0.9701        0.1185  2.9040\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer because the following parameters were re-set: momentum.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.5363       0.9314        0.2288  2.3480\n",
      "      2        0.2008       0.9508        0.1600  2.1730\n",
      "      3        0.1362       0.9599        0.1300  2.1460\n",
      "      4        0.0995       0.9654        0.1168  2.1610\n",
      "      5        0.0743       0.9678        0.1095  2.1820\n",
      "      6        0.0558       0.9690        0.1044  2.1420\n",
      "      7        0.0419       0.9713        0.1010  2.1110\n",
      "      8        0.0323       0.9693        0.1055  2.1270\n",
      "      9        0.0249       0.9714        0.1012  2.1110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10        0.0192       0.9685        0.1072  2.1350\n",
      "     11        0.0146       0.9714        0.0970  2.2160\n",
      "     12        0.0113       0.9731        0.0931  2.1110\n",
      "     13        0.0087       0.9730        0.0971  2.1090\n",
      "     14        0.0069       0.9734        0.0973  2.1020\n",
      "     15        0.0056       0.9731        0.0965  2.1530\n",
      "     16        0.0045       0.9735        0.0956  2.1540\n",
      "     17        0.0037       0.9729        0.0961  2.1340\n",
      "     18        0.0031       0.9735        0.0970  2.1550\n",
      "     19        0.0028       0.9734        0.0977  2.1180\n",
      "     20        0.0025       0.9733        0.0978  2.1750\n",
      "     21        0.0022       0.9733        0.0974  2.1480\n",
      "     22        0.0020       0.9740        0.0975  2.2220\n",
      "     23        0.0018       0.9736        0.0979  2.6770\n",
      "     24        0.0017       0.9736        0.0982  2.5870\n",
      "     25        0.0015       0.9738        0.0985  2.3720\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer because the following parameters were re-set: momentum.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.5408       0.9322        0.2333  2.1570\n",
      "      2        0.2023       0.9547        0.1591  2.1960\n",
      "      3        0.1358       0.9629        0.1267  2.1730\n",
      "      4        0.0967       0.9668        0.1119  2.1740\n",
      "      5        0.0708       0.9684        0.1044  2.2070\n",
      "      6        0.0535       0.9696        0.1014  2.2590\n",
      "      7        0.0414       0.9704        0.1002  2.1400\n",
      "      8        0.0327       0.9714        0.0981  2.2650\n",
      "      9        0.0257       0.9723        0.0954  2.4950\n",
      "     10        0.0195       0.9721        0.0943  2.6000\n",
      "     11        0.0148       0.9724        0.0925  2.4860\n",
      "     12        0.0111       0.9723        0.0946  2.5950\n",
      "     13        0.0086       0.9738        0.0930  2.5930\n",
      "     14        0.0067       0.9743        0.0925  2.5170\n",
      "     15        0.0055       0.9733        0.0924  2.5360\n",
      "     16        0.0045       0.9730        0.0935  2.5540\n",
      "     17        0.0039       0.9739        0.0930  2.5240\n",
      "     18        0.0034       0.9744        0.0925  2.2100\n",
      "     19        0.0030       0.9748        0.0918  2.2150\n",
      "     20        0.0027       0.9748        0.0916  2.2150\n",
      "     21        0.0023       0.9750        0.0918  2.3380\n",
      "     22        0.0021       0.9751        0.0921  2.3720\n",
      "     23        0.0019       0.9754        0.0922  2.3580\n",
      "     24        0.0017       0.9753        0.0922  2.2815\n",
      "     25        0.0016       0.9754        0.0922  2.2920\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer because the following parameters were re-set: momentum.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.6105       0.9124        0.2791  2.2929\n",
      "      2        0.2012       0.9449        0.1780  2.2400\n",
      "      3        0.1324       0.9540        0.1489  2.2840\n",
      "      4        0.0938       0.9573        0.1422  2.2410\n",
      "      5        0.0700       0.9593        0.1398  2.1970\n",
      "      6        0.0534       0.9604        0.1392  2.2370\n",
      "      7        0.0426       0.9630        0.1384  2.6371\n",
      "      8        0.0343       0.9606        0.1510  2.5180\n",
      "      9        0.0304       0.9628        0.1472  2.4530\n",
      "     10        0.0257       0.9625        0.1428  2.5025\n",
      "     11        0.0207       0.9614        0.1644  2.6910\n",
      "     12        0.0175       0.9608        0.1679  2.4300\n",
      "     13        0.0159       0.9628        0.1610  2.7410\n",
      "     14        0.0155       0.9619        0.1644  3.8630\n",
      "     15        0.0129       0.9650        0.1566  3.3000\n",
      "     16        0.0102       0.9611        0.1874  3.8640\n",
      "     17        0.0111       0.9679        0.1578  3.7960\n",
      "     18        0.0101       0.9684        0.1658  3.1800\n",
      "     19        0.0081       0.9689        0.1590  3.6540\n",
      "     20        0.0048       0.9703        0.1456  2.5630\n",
      "     21        0.0030       0.9703        0.1472  2.8120\n",
      "     22        0.0016       0.9701        0.1528  2.6540\n",
      "     23        0.0018       0.9729        0.1456  2.5470\n",
      "     24        0.0010       0.9723        0.1456  2.6740\n",
      "     25        0.0007       0.9733        0.1447  2.3680\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer because the following parameters were re-set: momentum.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.5722       0.9270        0.2269  2.5150\n",
      "      2        0.1926       0.9535        0.1521  2.9240\n",
      "      3        0.1283       0.9593        0.1294  2.4900\n",
      "      4        0.0922       0.9604        0.1230  2.8190\n",
      "      5        0.0694       0.9610        0.1281  2.4010\n",
      "      6        0.0561       0.9673        0.1116  2.5310\n",
      "      7        0.0470       0.9698        0.1108  2.4390\n",
      "      8        0.0409       0.9631        0.1378  2.5770\n",
      "      9        0.0353       0.9605        0.1456  2.5670\n",
      "     10        0.0288       0.9666        0.1248  3.3470\n",
      "     11        0.0232       0.9669        0.1180  3.4090\n",
      "     12        0.0177       0.9716        0.1119  2.8630\n",
      "     13        0.0125       0.9698        0.1282  2.6990\n",
      "     14        0.0081       0.9709        0.1347  2.5150\n",
      "     15        0.0065       0.9724        0.1233  2.5220\n",
      "     16        0.0050       0.9726        0.1242  2.4850\n",
      "     17        0.0052       0.9728        0.1310  2.8370\n",
      "     18        0.0035       0.9726        0.1322  2.9330\n",
      "     19        0.0021       0.9754        0.1254  3.0400\n",
      "     20        0.0013       0.9753        0.1179  2.4020\n",
      "     21        0.0009       0.9756        0.1185  2.4110\n",
      "     22        0.0007       0.9759        0.1194  2.6400\n",
      "     23        0.0006       0.9763        0.1203  2.9910\n",
      "     24        0.0006       0.9764        0.1210  2.5010\n",
      "     25        0.0005       0.9764        0.1217  2.4210\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: activation.\n",
      "Re-initializing optimizer because the following parameters were re-set: momentum.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.5845       0.9347        0.2317  2.6050\n",
      "      2        0.1908       0.9558        0.1536  2.5050\n",
      "      3        0.1247       0.9613        0.1331  2.6610\n",
      "      4        0.0917       0.9659        0.1132  2.6620\n",
      "      5        0.0726       0.9658        0.1133  2.8580\n",
      "      6        0.0574       0.9700        0.1044  3.4821\n",
      "      7        0.0442       0.9689        0.1092  3.2580\n",
      "      8        0.0360       0.9709        0.1108  2.8690\n",
      "      9        0.0273       0.9699        0.1074  2.9470\n",
      "     10        0.0245       0.9713        0.1076  2.6900\n",
      "     11        0.0217       0.9706        0.1140  2.7471\n",
      "     12        0.0192       0.9709        0.1190  3.2165\n",
      "     13        0.0156       0.9741        0.1063  3.7000\n",
      "     14        0.0107       0.9748        0.1131  3.3000\n",
      "     15        0.0061       0.9756        0.1082  3.2470\n",
      "     16        0.0041       0.9749        0.1173  3.2220\n",
      "     17        0.0030       0.9751        0.1130  2.9240\n",
      "     18        0.0023       0.9759        0.1137  2.5930\n",
      "     19        0.0017       0.9770        0.1103  2.8390\n",
      "     20        0.0013       0.9774        0.1113  2.6230\n",
      "     21        0.0009       0.9765        0.1115  2.5600\n",
      "     22        0.0008       0.9768        0.1118  2.5300\n",
      "     23        0.0007       0.9769        0.1121  2.7810\n",
      "     24        0.0006       0.9771        0.1128  2.6870\n",
      "     25        0.0006       0.9771        0.1135  3.0750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=NeuralNet(\n",
       "    (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (activation): ReLU()\n",
       "  ),\n",
       "),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'module__activation': [ReLU(), Tanh(),\n",
       "                                                LeakyReLU(negative_slope=0.01)]},\n",
       "             pre_dispatch='2*n_jobs', refit=False, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X.numpy(),y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_module__activation</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ReLU()</td>\n",
       "      <td>0.976905</td>\n",
       "      <td>0.972199</td>\n",
       "      <td>0.974496</td>\n",
       "      <td>0.974533</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tanh()</td>\n",
       "      <td>0.973205</td>\n",
       "      <td>0.970299</td>\n",
       "      <td>0.972646</td>\n",
       "      <td>0.972050</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LeakyReLU(negative_slope=0.01)</td>\n",
       "      <td>0.975705</td>\n",
       "      <td>0.973249</td>\n",
       "      <td>0.975096</td>\n",
       "      <td>0.974683</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         param_module__activation  split0_test_score  split1_test_score  \\\n",
       "0                          ReLU()           0.976905           0.972199   \n",
       "1                          Tanh()           0.973205           0.970299   \n",
       "2  LeakyReLU(negative_slope=0.01)           0.975705           0.973249   \n",
       "\n",
       "   split2_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.974496         0.974533        0.001921                2  \n",
       "1           0.972646         0.972050        0.001259                3  \n",
       "2           0.975096         0.974683        0.001044                1  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format the results such as to feed to HiPlot\n",
    "# torch.save(gs.cv_results_,'activation_CV.pt')\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(gs.cv_results_).iloc[:, [4,6,7,8,9,10,11]]\n",
    "results.param_module__activation = results.param_module__activation.astype(str)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'param_module__activation': 'ReLU()',\n",
       "  'split0_test_score': 0.9769046190761848,\n",
       "  'split1_test_score': 0.9721986099304966,\n",
       "  'split2_test_score': 0.9744961744261639,\n",
       "  'mean_test_score': 0.9745333333333334,\n",
       "  'std_test_score': 0.001921471837924856,\n",
       "  'rank_test_score': 2},\n",
       " {'param_module__activation': 'Tanh()',\n",
       "  'split0_test_score': 0.9732053589282144,\n",
       "  'split1_test_score': 0.9702985149257463,\n",
       "  'split2_test_score': 0.9726458968845327,\n",
       "  'mean_test_score': 0.97205,\n",
       "  'std_test_score': 0.0012593262268930255,\n",
       "  'rank_test_score': 3},\n",
       " {'param_module__activation': 'LeakyReLU(negative_slope=0.01)',\n",
       "  'split0_test_score': 0.9757048590281944,\n",
       "  'split1_test_score': 0.9732486624331217,\n",
       "  'split2_test_score': 0.9750962644396659,\n",
       "  'mean_test_score': 0.9746833333333333,\n",
       "  'std_test_score': 0.0010444117399852609,\n",
       "  'rank_test_score': 1}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for row in results.iterrows():\n",
    "    data.append(row[1].to_dict())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"wrap_html_f2fd2a\"><!DOCTYPE html>\n",
       "<html>\n",
       "  <head>\n",
       "    <meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/>\n",
       "    <title>HiPlot</title>\n",
       "    <link rel=\"icon\" href=\"static/icon.png\" />\n",
       "  </head>\n",
       "  <body style=\"margin:0px\">\n",
       "    <div id=\"hiplot_d1d1992ede2d4f9a8c1919e2dc37593f\" style=\"background-color: white\"><div style=\"text-align: center\">Loading HiPlot...</div>\n",
       "      <noscript>\n",
       "        HiPlot needs JavaScript to run\n",
       "      </noscript>\n",
       "    </div>\n",
       "\n",
       "    <script  type=\"text/javascript\" ></script>\n",
       "    <script type=\"text/javascript\">\n",
       "    function _hiplot_setup() {\n",
       "      if (typeof hiplot_setup == \"undefined\") {\n",
       "        console.info(\"waiting for HiPlot bundle to be loaded...\");\n",
       "        setTimeout(_hiplot_setup, 1000);\n",
       "        return;\n",
       "      }\n",
       "      var options = {\n",
       "        is_webserver: true,\n",
       "      };\n",
       "      /*ON_LOAD_SCRIPT_INJECT*/\n",
       "const comm_id = \"comm_1d3dfa\";\n",
       "try {\n",
       "    console.log(\"Setting up communication channel with Jupyter: \", comm_id);\n",
       "    var comm = Jupyter.notebook.kernel.comm_manager.new_comm(comm_id, {'type': 'hello'});\n",
       "    Object.assign(options, {\"comm\": comm});\n",
       "}\n",
       "catch(err) {\n",
       "    console.warn('Unable to create Javascript <-> Python communication channel' +\n",
       "        ' (are you in a Jupyter notebook? Jupyter labs is *not* supported!)');\n",
       "}\n",
       "        \n",
       "        Object.assign(options, eval('(' + \"{\\\"is_webserver\\\": false, \\\"experiment\\\": {\\\"datapoints\\\": [{\\\"uid\\\": \\\"0\\\", \\\"values\\\": {\\\"param_module__activation\\\": \\\"ReLU()\\\", \\\"split0_test_score\\\": 0.9769046190761848, \\\"split1_test_score\\\": 0.9721986099304966, \\\"split2_test_score\\\": 0.9744961744261639, \\\"mean_test_score\\\": 0.9745333333333334, \\\"std_test_score\\\": 0.001921471837924856, \\\"rank_test_score\\\": 2}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"1\\\", \\\"values\\\": {\\\"param_module__activation\\\": \\\"Tanh()\\\", \\\"split0_test_score\\\": 0.9732053589282144, \\\"split1_test_score\\\": 0.9702985149257463, \\\"split2_test_score\\\": 0.9726458968845327, \\\"mean_test_score\\\": 0.97205, \\\"std_test_score\\\": 0.0012593262268930255, \\\"rank_test_score\\\": 3}, \\\"from_uid\\\": null}, {\\\"uid\\\": \\\"2\\\", \\\"values\\\": {\\\"param_module__activation\\\": \\\"LeakyReLU(negative_slope=0.01)\\\", \\\"split0_test_score\\\": 0.9757048590281944, \\\"split1_test_score\\\": 0.9732486624331217, \\\"split2_test_score\\\": 0.9750962644396659, \\\"mean_test_score\\\": 0.9746833333333333, \\\"std_test_score\\\": 0.0010444117399852609, \\\"rank_test_score\\\": 1}, \\\"from_uid\\\": null}], \\\"parameters_definition\\\": {}, \\\"_displays\\\": {\\\"PARALLEL_PLOT\\\": {}, \\\"TABLE\\\": {}, \\\"XY\\\": {}}}, \\\"persistent_state\\\": null}\" + ')'));\n",
       "        \n",
       "      var hiplot_instance = hiplot_setup(document.getElementById(\"hiplot_d1d1992ede2d4f9a8c1919e2dc37593f\"), options);\n",
       "      /*AFTER_SETUP_SCRIPT_INJECT*/\n",
       "    }\n",
       "    if (document.readyState == \"complete\") {\n",
       "      _hiplot_setup();\n",
       "    }\n",
       "    else {\n",
       "      document.addEventListener(\"DOMContentLoaded\", _hiplot_setup);\n",
       "    }\n",
       "    </script>\n",
       "  </body>\n",
       "</html></div>\n",
       "<script type=\"text/javascript\">\n",
       "(function() {\n",
       "    const elem = document.getElementById(\"wrap_html_f2fd2a\");\n",
       "    elem.style.width = \"100vw\";\n",
       "    const removeElems = elem.parentElement.parentElement.getElementsByClassName(\"prompt\");\n",
       "    for (var i = 0; i < removeElems.length; ++i) {\n",
       "        removeElems[i].remove();\n",
       "    }\n",
       "    elem.parentElement.style.overflowX = \"visible\";\n",
       "\n",
       "    const scale_to_100pct_screen = function() {\n",
       "        elem.style.marginLeft = '0px';\n",
       "        elem.style.marginLeft = (- elem.getBoundingClientRect().x) + 'px';\n",
       "    };\n",
       "    window.addEventListener('resize', function() {\n",
       "        scale_to_100pct_screen();\n",
       "    });\n",
       "    scale_to_100pct_screen();\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<hiplot.ipython.IPythonExperimentDisplayed at 0x2c1a6aa2828>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hip.Experiment.from_iterable(data).display(force_full_width = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
